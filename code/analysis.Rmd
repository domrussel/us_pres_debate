---
title: "A Textual Analysis of Every U.S. General Election Debate"
author: "Dominic Russel"
date: "10/19/2020"
output:
  html_document:
    theme: cosmo
---

```{r setup, include=FALSE}

# Change this line to true to instead include all code
knitr::opts_chunk$set(echo = FALSE)

```


```{r results='hide',message=FALSE}
library(tidyverse)
library(magrittr)
library(lubridate)
library(tidytext)
library(ggrepel)
library(ggwordcloud)
library(grid)
library(gridExtra)
library(sentimentr)
library(cleanNLP)
library(knitr)
library(kableExtra)

dat_in <- read_csv("../output/all_debate_text.csv")

dat_main <- dat_in %>% 
  filter(speaker_clean == gop_candidate |
           speaker_clean == dem_candidate | 
           speaker_clean == other_candidate) %>% 
  mutate(party = case_when(
    speaker_clean == gop_candidate ~ "Republican Candidate",
    speaker_clean == dem_candidate ~ "Democratic Candidate",
    speaker_clean == other_candidate ~ "Other Candidate"
  )) %>% 
  mutate(speaker_clean = case_when(
    speaker_clean == "B_CLINTON" ~ "B. Clinton",
    speaker_clean == "H_CLINTON" ~ "H. Clinton",
    speaker_clean == "GHW_BUSH" ~ "G.H.W. Bush",
    speaker_clean == "GW_BUSH" ~ "G.W. Bush",
    speaker_clean == "MCCAIN" ~ "McCain",
    TRUE ~ str_to_title(speaker_clean)
  ))
```

The first U.S. Presidential debate was [widely covered](https://www.washingtonpost.com/entertainment/tv/trump-biden-debate-tv-review/2020/09/30/f5217482-020d-11eb-a2db-417cddf4816a_story.html) as [one of the worst in U.S. history](https://theweek.com/articles/940566/worst-presidential-debate-all-time), with analysts [like CNN's Jake Tapper](https://www.youtube.com/watch?v=pKys4M-bi9g) left grasping for the words to describe it. Was it truly "[a hot mess, inside a dumpster fire, inside a trainwreck](https://www.youtube.com/watch?v=pKys4M-bi9g)"?  Using textual analyses, I attempt to quantitatively assess this debate, and every other, in U.S. history.

My results are built on a dataset of every general election Presidential and Vice Presidential debate in U.S. history. I scraped this data from the [Commission on Presidential Debates website](https://www.debates.org/voter-education/debate-transcripts/) and [rev.com](https://www.rev.com/blog/transcript-category/debate-transcripts). The final dataset includes over 13,500 questions and responses organized by debate and speaker, from the first Kennedy-Nixon debate in 1960 to Harris-Pence VP debate in 2020. You can find the data [here]( https://github.com/domrussel/us_pres_debate).

```{r results='hide',message=FALSE}
word_sentiment <- dat_main %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  inner_join(get_sentiments("afinn"))
```

```{r results='hide',message=FALSE}
# This is a helper function that will make the metrics over time
# plots. The idea to assist ggrepel by putting a bunch of points on
# the line comes from: https://github.com/slowkow/ggrepel/issues/110
make_over_time_plot <- function(dat, x_col, y_col, label_col, color_col, y_label,
                                y_min, y_max,
                                x_nudge_col=NULL, y_nudge_col=NULL){
  
  dem_dat <- filter(dat, party=="Democratic Candidate")
  rep_dat <- filter(dat, party=="Republican Candidate")
  
  data_text <- rbind(
    as.data.frame(do.call(cbind, approx(dem_dat[[x_col]], dem_dat[[y_col]], n = 1000))),
    as.data.frame(do.call(cbind, approx(rep_dat[[x_col]], rep_dat[[y_col]], n = 1000))))
  
  data_text[[label_col]] <- ""
  data_text[[color_col]] <- NA_character_
  colnames(data_text) <- c(x_col, y_col, label_col, color_col)
  
  
  data_text$x_nudge <- 0
  data_text$y_nudge <- 0
  
  if(!is.null(x_nudge_col)){
    dat <- dat %>% 
      rename("x_nudge"=x_nudge_col)
  }
  else{
    dat$x_nudge <- 0
  }
  
  if(!is.null(y_nudge_col)){
    dat <- dat %>% 
      rename("y_nudge"=y_nudge_col)
  }
  else{
    dat$y_nudge <- 0
  }
  
  data_text <- rbind(dat, data_text)
  
  ggplot(dat, aes_string(x=x_col, y=y_col, col=color_col, label=label_col)) +
    geom_line(size=1.25) +
    geom_point(size=3) +
    geom_text_repel(data=data_text,
                    segment.size  = 0.2,
                    box.padding = 0.75,
                    show.legend = F,
                    nudge_x = data_text$x_nudge,
                    nudge_y = data_text$y_nudge,
                    force=1) +
    xlim(1958,2022) +
    ylim(y_min,y_max) +
    theme_bw() +
    theme(legend.title = element_blank()) +
    scale_color_manual(values=c("#0015BC", "#DE0100")) +
    labs(x="Election Year", y=y_label)
}
```

## Simple Sentiment Analysis

One way to analyze these data is to look at how positive or negative the candidates are in their responses --- the sentiment of their arguments.  In its most simple form, a sentiment analysis counts the number of positive words (“excellent”, “brilliant”, “win”) and negative words (“worst”, “horrible”, “fraud”) used in a text. A text with more positive than negative words will have a more positive average sentiment (and vice versa).

Using the [AFINN](https://github.com/fnielsen/afinn) dictionary, which assigns scores of -5 to 5 to a corpus of nearly 2,500 words, I calculate the average sentiment of every major party presidential debate candidate. The plot below shows the average sentiment of presidential debaters from the two major parties.

#### Average Sentiment of U.S. Presidential Debaters

```{r message=FALSE, fig.width=12, fig.height=6}
dat_p1 <- word_sentiment %>% 
  # Here, drop the VP debates and other party candidates
  filter(vp_debate == 0, party != "Other Candidate") %>% 
  group_by(election, party, speaker_clean) %>% 
  summarise(avg_word_sent = mean(value)) %>% 
  ungroup %>% 
  # Some nudge columns that will be useful when we plot
  mutate(x_nudge = case_when(
    speaker_clean == "G.H.W. Bush" & election == 1992 ~ 2,
    speaker_clean == "H. Clinton" & election == 2016 ~ 5,
    TRUE ~ 0
  )) %>% 
  mutate(y_nudge = case_when(
    speaker_clean == "B. Clinton" & election == 1992 ~ -0.1,
    TRUE ~ 0
  ))


make_over_time_plot(dat_p1, x_col="election", y_col="avg_word_sent",
                    label_col="speaker_clean", color_col="party", y_label="Average Sentiment",
                    y_min=-0.6, y_max=0.7,
                    x_nudge_col="x_nudge", y_nudge_col="y_nudge")
```

At this point, it is important to note that sentiment analysis is an imperfect science, and that any results are dependent on certain assumptions about language. Using the same dictionary of words over time, for example, might be problematic if a word is more emotionally charged today than in the past. Furthermore, any textual analysis might miss important information about factors like the debaters' tone or non-verbal actions.

Even with these caveats in mind, Trump clearly stands out as a historically negative debater. The average sentiment of his performances is more than twice as negative than even the closest comparisons (Reagan in 1980 and 1984, and Biden in 2020). By contrast, the most positive debate performances were delivered by Gerald Ford, each of the George Bushes in their first terms, and both John F. Kennedy and Richard Nixon in the 1960 debates. In general, sentiment appears to have declined over time from the first two years of presidential debates in 1960 and 1976 (there were no presidential debates in the 1964, 1968, and 1972 elections) and the last two in 2016 and 2020.

We can also break out sentiment by each individual debate performance. Here, I include VP debates and the three non-major party candidates that made the debate stage. The graph  shows that the four most negative debate performances in U.S. history all belong to Trump.

#### Average Sentiment by U.S. General Election Debate Performance

```{r message=FALSE, fig.width=8, fig.height=8}
set.seed(9)

dat_p2 <- word_sentiment %>% 
  group_by(date, party, speaker_clean) %>% 
  summarise(avg_word_sent = mean(value)) %>% 
  ungroup() %>% 
  mutate(party = factor(party, levels=c("Democratic Candidate",
                                        "Republican Candidate",
                                        "Other Candidate"))) %>% 
  group_by(speaker_clean) %>% 
  mutate(rand_int=sample.int(n())) %>% 
  ungroup %>% 
  mutate(text_label = if_else(
    date >= as.Date("2016-01-01") | rand_int == 1 | avg_word_sent < -0.3 | avg_word_sent > 0.6,
    paste(speaker_clean, date), ""))

ggplot(dat_p2, aes(x=1, y=avg_word_sent, col=party, label=text_label)) +
  geom_vline(xintercept = 1, color="gray") +
  geom_point(size=2, alpha=0.35) +
  geom_text_repel(
    size = 3,
    segment.size = 0.2,
    point.padding = 0.5,
    min.segment.length = 0,
    show.legend = F
    ) +
  theme_minimal() +
  theme(
    axis.line.x  = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x  = element_blank(),
    axis.title.x = element_blank(),
    panel.grid.major.x = element_blank(),
    legend.title = element_blank()
  ) + 
  scale_color_manual(values=c("#0015BC", "#DE0100", "#8A8A8A")) +
  labs(y="Average Sentiment")
```

Why are Trump's debates rated as so negative? Our simple dictionary-based method allows us to easily look "under the hood" and find out. The tables below compare Trump's most emotionally charged words with those of John F. Kennedy, one of the more positive debaters. The <span style="color:#900C3F;">purple words</span> are negative, arranged by sentiment value then by usage. The <span style="color:#0b8a61;">green words</span> are positive, arranged similarly so that the most positive, most used words are at the bottom. The farthest right column shows how many times Trump/Kennedy used each word as a percent of all their positive and negative words.

```{r message=FALSE}
trump_word_usage <- word_sentiment%>% 
  filter(speaker_clean == "Trump") %>% 
  group_by(word, value) %>% 
  summarise(n=n()) %>% 
  ungroup %>% 
  mutate(share_of_words = round((n/sum(n)) * 100, 2)) 

kennedy_word_usage<- word_sentiment %>% 
  filter(speaker_clean == "Kennedy") %>% 
  group_by(word, value) %>% 
  summarise(n=n()) %>% 
  ungroup %>% 
  mutate(share_of_words = round((n/sum(n)) * 100, 2)) 

bind_rows(
  trump_word_usage %>% arrange(value, desc(share_of_words)) %>% head(10),
  trump_word_usage %>% arrange(value, share_of_words) %>% tail(10)) %>% 
  select(Word=word, `Sentiment Value`=value, `Share of Neg & Pos Words (%)`= share_of_words) %>% 
  kbl(caption="Trump Most Used Negative & Positive Words") %>%
  kable_styling(full_width = F, position = "float_left", font_size=14) %>%
  row_spec(1:10, color = "#900C3F") %>% 
  row_spec(11:20, color = "#0b8a61")


bind_rows(
  kennedy_word_usage %>% arrange(value, desc(share_of_words)) %>% head(10),
  kennedy_word_usage %>% arrange(value, share_of_words) %>% tail(10)) %>% 
  select(Word=word, `Sentiment Value`=value, `Share of Neg & Pos Words (%)`= share_of_words) %>% 
  kbl(caption="Kennedy Most Used Negative & Positive Words") %>% 
  kable_styling(full_width = F, position = "float_right", font_size=14) %>%
  row_spec(1:10, color = "#900C3F") %>% 
  row_spec(11:20, color = "#0b8a61")

```
<div style="margin-bottom:850px;">
</div>

Trump's words "bastards", "fraud", "hell", "catastrophic", and "fraudulent" all had lower sentiment scores than any word Kennedy used in his four debates. In addition, Trump's frequent usage of the word "bad" (which alone makes up nearly five percent of his negative/positive words) drags his sentiment score down. While Trump does also use more very emotionally charged *positive* words than Kennedy (in particular "wonderful" and variations of the word "win"), Kennedy also often used a number of words scored as fairly positive like "successful", "breakthrough", and "win".

A final thought on these analyses: while the first plot showed that the sentiment in Biden's first 2020 debate performance was one of the most negative compared to the average of past candidates, Biden's individual past debate performances haven't been particularly negative. The graph below highlights Trump's four presidential debates and Biden's two VP debates (in 2008 and 2012) and one presidential debate.

#### Average Sentiment by U.S. General Election Debate Performance -- Biden/Trump

```{r message=FALSE, fig.width=8, fig.height=8}
set.seed(9)

dat_p3 <- dat_p2 %>% 
  mutate(text_label = if_else(
    speaker_clean %in% c("Trump", "Biden"),paste(speaker_clean, date), ""))

ggplot(dat_p3, aes(x=1, y=avg_word_sent, col=party, label=text_label)) +
  geom_vline(xintercept = 1, color="gray") +
  geom_point(size=2, alpha=0.35) +
  geom_text_repel(
    size = 3,
    segment.size = 0.2,
    point.padding = 0.5,
    min.segment.length = 0,
    show.legend = F
    ) +
  theme_minimal() +
  theme(
    axis.line.x  = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x  = element_blank(),
    axis.title.x = element_blank(),
    panel.grid.major.x = element_blank(),
    legend.title = element_blank()
  ) + 
  scale_color_manual(values=c("#0015BC", "#DE0100", "#8A8A8A")) +
  labs(y="Average Sentiment")
```

## Additional Measures of Sentiment and Emotion

One potential problem with the simple dictionary-based method used in the previous analysis is that it could overlook the presence of "valence shifter" words, like "not" and "very", that could change a sentence's meaning or intensity. For example, if someone used the word "bad" we might want to know if they said something was "not bad" or "very bad". Another thing we might be interested in is looking beyond simply a negative/positive scale, and instead at specific emotions like anger or disgust.

The [sentimentr](https://github.com/trinker/sentimentr) package implements solutions for both. Without going too far into the details, the package uses a similar set of dictionary methods but with an expanded vocabulary, accounting for valence shifters, and word tagging for particular emotions.

Using this package, we can first re-create our graph of debater sentiment over time for the two major parties. While some of the trends shift, our main takeaway clearly holds: Trump is a historically negative debater.

#### Average Sentiment of U.S. Presidential Debaters (sentimentr)

```{r message=FALSE, fig.width=12, fig.height=6}
sentence_sent <- dat_main %>% 
  filter(vp_debate == 0, party != "Other Candidate") %>% 
  group_by(election, speaker_clean, party) %>% 
  summarise(text = paste(text, collapse=" ")) %>% 
  ungroup %>% 
  get_sentences() %$%
  sentiment_by(text, list(election, speaker_clean, party)) %>% 
  as_tibble() %>% 
  select(-word_count, -sd)

make_over_time_plot(sentence_sent, x_col="election", y_col="ave_sentiment",
                    label_col="speaker_clean", color_col="party", y_label="Average Sentiment",
                    y_min=-0.02, y_max=0.14)
```

We can then look at particular emotions expressed, beyond simply negative or positive sentiment. This analysis essentially works the same way --- certain words are tagged as corresponding to a particular emotion. The more often these words are used, the stronger the average emotional score. Here, I look at four emotions: disgust, anger, fear, and sentiment. 

#### Average Emotional Scores of U.S. Presidential Debaters (sentimentr)

```{r message=FALSE, fig.width=16, fig.height=12}
sentence_emotion <- dat_main %>% 
  filter(vp_debate == 0, party != "Other Candidate") %>% 
  group_by(election, speaker_clean, party) %>% 
  summarise(text = paste(text, collapse=" ")) %>% 
  ungroup %>% 
  get_sentences() %$%
  emotion_by(text, list(election, speaker_clean, party)) %>% 
  as_tibble() %>% 
  select(-word_count, -emotion_count, -sd)


disgust <- 
  make_over_time_plot(select(filter(sentence_emotion, emotion_type=="disgust"), -emotion_type),
                    x_col="election", y_col="ave_emotion",
                    label_col="speaker_clean", color_col="party", y_label="",
                    y_min=0, y_max=0.014)
anger <- 
  make_over_time_plot(select(filter(sentence_emotion, emotion_type=="anger"), -emotion_type),
                    x_col="election", y_col="ave_emotion",
                    label_col="speaker_clean", color_col="party", y_label="",
                    y_min=0.005, y_max=0.02)

fear <- 
  make_over_time_plot(select(filter(sentence_emotion, emotion_type=="fear"), -emotion_type),
                    x_col="election", y_col="ave_emotion",
                    label_col="speaker_clean", color_col="party", y_label="",
                    y_min=0.005, y_max=0.03)

trust <- 
  make_over_time_plot(select(filter(sentence_emotion, emotion_type=="trust"), -emotion_type),
                    x_col="election", y_col="ave_emotion",
                    label_col="speaker_clean", color_col="party", y_label="",
                    y_min=0.015, y_max=0.06)


grid.arrange(
  disgust + labs(title="Disgust") +
    theme(legend.position = "none", plot.title = element_text(size=16)),
  anger + 
    labs(title = "Anger") + 
    theme(legend.position = "none", plot.title = element_text(size=16)),
  fear + 
    labs(title = "Fear") +
    theme(legend.position = "none", plot.title = element_text(size=16)),
  trust + 
    labs(title = "Trust") +
    theme(legend.position = "none", plot.title = element_text(size=16)),
  nrow = 2)

```

The graphs show that Trump's 2016 debates ranked the highest in disgust and anger, while his 2020 debate ranked the lowest in trust. George W. Bush's 2004 debate performances, which included discussions of 9-11 and the "war on terror" is ranked as the most fearful.

## Most Commonly Used Bigrams

To better understand the topics discussed in each series of debates, I next construct a series of simple word clouds for each election (where word size corresponds to how frequently the word is used). Instead of using individual words, I use bigrams, sequences of two adjacent words. These allow us to better extract complete themes.

Below are clouds from 1960, 1984, 2004, 2016, and 2020. Each provides a window into the issues that were relevant at the time. In 1960 and 1984, for example, foreign policy discussions surrounding the Soviet Union and Latin and South America played a prominent role in the debates. In 2004, debates around health care and Middle East foreign policy took center stage.

You can find clouds for every year of presidential debates [here](https://github.com/domrussel/us_pres_debate/tree/master/output).

```{r message=FALSE, fig.width=10, fig.height=4}
set.seed(17)

bigrams <- dat_main %>% 
  filter(vp_debate == 0, party != "Other Candidate") %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(!is.na(word1), !is.na(word2)) %>% 
  filter(word1 != "uh", word2 != "uh")

bigram_frequencies <- bigrams_filtered %>%
  group_by(election, speaker_clean, word1, word2) %>% 
  summarise(n = n()) %>% 
  ungroup %>%
  arrange(desc(n)) %>% 
  group_by(election, speaker_clean) %>% 
  slice(1:20) %>% 
  ungroup

bigram_frequencies_w_party <- dat_main %>% 
  distinct(election, speaker_clean, party) %>% 
  right_join(bigram_frequencies, by=c("election","speaker_clean")) %>% 
  mutate(n_gram = paste(word1, word2))

dems <- filter(bigram_frequencies_w_party, party == "Democratic Candidate")
repubs <- filter(bigram_frequencies_w_party, party == "Republican Candidate")

# years <- unique(bigram_frequencies_w_party$election)
years <- c(1960, 1984, 2004, 2016, 2020)

for(i in 1:length(years)){
  
  year <- years[i]
  
  curr_dem <- filter(dems, election == year)
    # Sometimes the speaker says his opponent's name way more than
    # all the others (e.g. Governor Romney). This leads the cloud
    # to have a weird scaling. We can still show that the word
    # was said far more but not destroy the cloud by scaling
    # it down a bit.
  curr_dem <- curr_dem %>% arrange(desc(n))
  if(curr_dem$n[1] > 1.25 * curr_dem$n[2]){
    curr_dem$n[1] <- curr_dem$n[2] * 1.25
  }
  curr_dem_name <- curr_dem$speaker_clean[1]
  
  curr_repub <- filter(repubs, election == year)
  curr_repub <- curr_repub %>% arrange(desc(n))
  if(curr_repub$n[1] > 1.25 * curr_repub$n[2]){
    curr_repub$n[1] <- curr_repub$n[2] * 1.25
  }
  curr_repub_name <- curr_repub$speaker_clean[1]
  
  p_dem <- ggplot(curr_dem, aes(label=n_gram, size=n)) +
    geom_text_wordcloud(color="#0015BC") +
    scale_size_area(max_size = 6) +
    theme_minimal() +
    theme(plot.title = element_text(size=14,
                                    hjust=0.5,
                                    color = "#3F3F3F")) +
    labs(title=curr_dem_name)
  
  p_repub <- ggplot(curr_repub, aes(label=n_gram, size=n)) +
    geom_text_wordcloud(color="#DE0100") +
    scale_size_area(max_size = 6) +
    theme_minimal() +
    theme(plot.title = element_text(size=14,
                                    hjust=0.5,
                                    color = "#3F3F3F")) +
    labs(title=curr_repub_name)
  
  p_both <- grid.arrange(p_dem,
                         p_repub,
                         nrow = 1,
                         top = textGrob(str_interp("${year} Election Debates"),
                                        gp=gpar(fontsize=18, col="#3F3F3F")))
  
  p_both
}
```

## Principal Components Analysis

In my final analysis, I convert each candidate's entire debate text into a two-dimensional measure. While this measure will not be easily interpretable, we can think of candidates that are "closer" together on these two dimensions as using more similar language. This sounds like magic, but is really a two step process:

1. Create a table where every row represents a speaker and every column represents a row. The value of each cell is the number of times the speaker in the row $n$ uses the word in column $m$ (here, I normalize this number by taking the log).^[To focus on data that provide the most useful information, we can exclude words that appear in very many debates (thus not providing unique information) or very few debates (thus telling us little about how the documents relate to each other). In my analysis I exclude words that appear in more than 90% or less than 10% of speakers' debate performances.]

2. Use a [Principal Components Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) to "project" this very big $N$x$M$ table into an $N$x$2$ table that captures as much of the information from the original table as possible.^[This might still sound like magic, but I promise is just linear algebra.]

The graphs below show these principal components for every debate performance. Again, the key thing to remember is just that points that are closer together were found to be more similar using this technique.

The first graph plots these components from data that includes the nouns, adjectives, and verbs each speaker used (I exclude proper nouns, such as "Obama", which are likely to be time-period specific). 

#### Principal Components Analysis (Nouns, Adjectives, and Verbs)
```{r message=FALSE, fig.width=12, fig.height=8}
# I pre-annotated the using cleanNLP. This process takes a long time.
# See the script clean_NLP_annotate.R.

anno <- read_rds("../output/annotated_all_debates.Rds")

# Do PCA using all (non-proper) Nouns, Adjectives, and Verbs
pca <- anno$token %>%
  filter(upos %in% c("NOUN", "ADJ", "VERB")) %>%
  cnlp_utils_tfidf(min_df = 0.1, max_df = 0.9, tf_weight = "lognorm") %>%
  cnlp_utils_pca()

pca <- bind_cols(anno$document, pca) %>% 
  mutate(party=factor(party, levels=c("Democratic Candidate",
                                      "Republican Candidate",
                                      "Other Candidate")))

ggplot(pca, aes(PC1, PC2, label=paste(paste(speaker_clean, date)), color=party)) +
  geom_point(alpha = 0.35, size = 4) +
  geom_text_repel(
    size = 3,
    segment.size = 0.2,
    min.segment.length = 0,
    show.legend = F
  ) +
  scale_color_manual(values=c("#0015BC", "#DE0100", "#8A8A8A")) +
  theme_void() +
  theme(legend.title = element_blank())
  
```

On the bottom, one set of debate performances sticks out from the others: George W. Bush and John Kerry's September 30, 2004 debate. A likely reason? The candidates agreed to focus this debate entirely on foreign policy and homeland security, shaping the vocabularly they used. The 2016 Pence-Kaine debate, the two closest performances, also included lengthy dicussions on a number of foreign policy and national defense topics.

We also see that the performances grouped together on the left side of the plot are generally from the more distant past. In particular, the far upper left includes every debate performance from Kennedy, Nixon, Ford, and Carter. By contrast, the debate performances in the top right appear to be primarily from the late 90s and early 2000s.

While the exclusion of proper nouns may have reduced the influence of some time-specific trends, the relative importance of certain issues (e.g. inflation, health care) are still likely to vary over time and shape the nouns used. To avoid picking up these time-specific topics, I re-run the analysis including only adjectives and verbs.

#### Principal Components Analysis (Adjectives and Verbs)
```{r message=FALSE, fig.width=12, fig.height=8}
# Do PCA using only Adjectives and Verbs
pca2 <- anno$token %>%
  filter(upos %in% c("ADJ", "VERB")) %>%
  cnlp_utils_tfidf(min_df = 0.1, max_df = 0.9, tf_weight = "lognorm") %>%
  cnlp_utils_pca()

pca2 <- bind_cols(anno$document, pca2) %>% 
  mutate(party=factor(party, levels=c("Democratic Candidate",
                                      "Republican Candidate",
                                      "Other Candidate")))

ggplot(pca2, aes(PC1, PC2, label=paste(paste(speaker_clean, date)), color=party)) +
  geom_point(alpha = 0.35, size = 4) +
  geom_text_repel(
    size = 3,
    segment.size = 0.2,
    min.segment.length = 0,
    show.legend = F
  ) +
  scale_color_manual(values=c("#0015BC", "#DE0100", "#8A8A8A")) +
  theme_void() +
  theme(legend.title = element_blank(),
        plot.title = element_text(hjust=0.5))
  
```

While many the earliest debates remain off to the left, a new outlying cluster appears in the bottom right: every debate performance of Donald Trump. His debate performances are, well, quantifiably unique.

Thanks for reading! If you enjoyed this please share it. You can find out the code for this document, as well as the cleaned data in [this Github repository](https://github.com/domrussel/us_pres_debate). 